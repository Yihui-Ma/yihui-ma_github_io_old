<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>YIHUI</title><meta name="author" content="Yihui-Ma"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.2.0"><link rel="alternate" href="/atom.xml" title="YIHUI" type="application/atom+xml">
</head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">YIHUI</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Yihui-Ma</h3><p class="author-bio">Your biography can be writed down here.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">吴恩达——机器学习系列课程 笔记</h2><article><p>该笔记为观看视频个人笔记。</p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>&emsp;&emsp;监督学习算法是指我们给算法一个数据集，其中包含了正确答案，算法的目的就是给出更多的正确答案。</p>
<h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>&emsp;&emsp;分类问题——设法预测一个离散值输出，0或1；有时可能有两种以上的分类。</p>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>&emsp;&emsp;数据没有任何标签，数据都具有相同的标签或者没有标签。对于给定的数据集，无监督学习算法可能给定该数据包含的两个不同的簇，可以把这些数据分成两个不同的簇，这就是聚类算法；其中一个应用聚类算法的例子是谷歌新闻。无监督学习在其他领域的应用，有如被用来组织大型的计算机集群，管理大型数据中心（大型计算机集群）；用于社交网络的分析；市场细分中的应用，许多公司拥有庞大的客户信息数据库，但预先并不知道有哪些细分市场；用于天文数据分析，这些聚类算法带来了星系形成理论；这些都是聚类，聚类只是无监督学习的一种。<br><strong>鸡尾酒算法</strong><br>[W,s,v]&#x3D;svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&#39;);<br>svd: 是奇异值分解的缩写</p>
<h2 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>&emsp;&emsp;回归是指我们预测一个具体的数值输出。另一种最常见的监督学习问题，被称为分类问题，我们用它来预测离散值输出。一元线性回归模型又称为单变量线性回归模型。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>&emsp;&emsp;代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。事实上，求出误差的平方和的原因，在于误差平方代价函数对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段，对于大多数线性回归问题是非常合理的。<br>&emsp;&emsp;当参数只有一个时，代价函数为二维碗状函数；当参数有两个时，代价函数为三维碗状函数；可以将三维碗状函数图转换为等高图。<br>$\displaystyle minimize_{\theta_{0}\theta_{1}}\frac{1}{2m}\sum^{m}_{i&#x3D;1}(h_\theta(x^{(i)}-y^{(i)}))^2$</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>&emsp;&emsp; repeat until convergence { $ \theta_j &#x3D; \theta_j -\alpha \frac{\partial}{\partial \theta_j}$}。在matlab中，$:&#x3D;$是赋值，$&#x3D;$是判断值是否相等。如果$\alpha$很大，梯度下降就很迅速，反之很小。这个公式的微妙之处在于需要同时更新$\theta_0$和$\theta_1$。在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，根据定义，在局部最低时导数等于零。所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的运动方式。所以实际上没有必要再另外减小$\alpha$。</p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2024 by Yihui-Ma</div><div class="theme-info">Powered by <a href="https://hexo.io" rel="nofollow">Hexo</a> & <a href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>